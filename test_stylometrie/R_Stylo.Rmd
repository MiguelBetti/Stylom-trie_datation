---
title: "Stylométrie et datation (UniGE, 26.04.2024)"
author: "Miguel Betti"
Basé sur les cours de Simon Gabay (Distant reading I, UniGE, 2021-2022)
---

Il existe de multiples options informatiques pour faire de la stylométrie. Nous nous proposons, dans ce cours, d'utiliser le package `R` nommé  *Stylo*.

Demonstration avec un petit corpus de contes et nouvelles en espagnol écrits par des auteurs argentins du XXe siècle:

Horacio Quiroga (1878-1937)

Jorge Luis Borges (1899-1986)

Adolfo Bioy Casares (1914-1999)

Julio Cortázar (1914-1984)

Nous allons faire un test avec la nouvelle "Las doce figuras del mundo", écrit en collaboration par Jorge Luis Borges et Adolfo Bioy Casares, mais publiée avec le pseudonyme de H. Bustos Domecq.


# 1. Préparation de la session de travail

# 1.1. Corpus et session de travail

`Stylo` a besoin de savoir où chercher les données du "corpus" qu'il va utiliser, et celles-ci doivent **impérativement** obéir à trois règles simples:

a. Tous les textes qui forment notre corpus de travail doivent se trouver dans un dossier intitulé *corpus*.
b. Ce dossier `corpus` ne doit contenir que les fichiers du corpus: tout autre document se retrouverait analysé avec le reste, et perturberait les résultats.
3. Les fichiers du corpus doivent utiliser un même format, soit en `txt`, soit en `XML`, soit en `HTML` (dans notre cas, `txt`).

Nous devons préparer la session de travail, en désignant à `R` le fichier de notre ordinateur à partir duquel nous allons travailler pour qu'il trouve notre fichier "corpus", et qu'il y sauvegarde nos résultats.

Pour indiquer où se trouve le fichier de travail, deux solutions sont possibles:

* ou bien *via* le menu avec `Session>Set Working Directory>Choose Directory`
* ou bien directement dans `R` avec la commande `setwd` (pour *Set working directory*), qui s'utilise de cette manière :

```{r}
setwd("/Users/miguel/Desktop/Stylometrie_datation/test_stylometrie")
```

Nous pouvons vérifier que nous sommes dans le bon dossier de travail de cette manière :

```{r}
getwd()
```


# 1.2. Installation de la librairie "stylo()"

```{r}
if(!require("stylo")){
  install.packages("stylo")
  library(stylo)
}
```
# 2. Le *data clustering*

## 2.1 Une première analyse de cluster avec mon corpus

Le *data clustering* ("partitionnement de données" en français) cherche à diviser un ensemble de données en différents "groupes" homogènes selon des caractéristiques qu'ils partagent. Ces groupes sont formés à partir de calculs qui déterminent leur proximité (similarité ou distance).

[l'option "gui=TRUE" nous donne accès à la console, mais nous pouvons le faire aussi manuellement "gui=FALSE"]

1. Je charge mon corpus: corpus.dir = "localisation du dossier"
2. Je détermine:
- la langue de travail corpus.lang = "langue choisie"
- les données à analyser analyzed.features = mots ("w"), chaînes de caractères ("chars"), etc.
- le nombre de mots ou caractères les plus fréquents: mfw.min  = "100"
- le type d'analyse: analysis.type = cluster ("CA"), PCA, etc., et la distance: Manhattan, Classic Delta, etc.
- l'output

On obtiendra un graphique (dendogramme), qui est accompagné de quelques informations en bas de page (`100 MFW Culled @ 0% Manhattan distance`). Le *culling est à 0%*, ce qui signifie que les mots les plus fréquents peuvent être absents de certains textes. Un culling à 100% signifierait que nous ne retiendrions que les mots les plus fréquents retenus dans **tous** les textes.

Tous les labels (les noms des fichiers formant le corpus) ont la même chaîne de caractères avant l'underscore (dans notre cas, le nom chaque auteur), afin de simplifier la lecture des résultats.

Nous pouvons essayer plusieurs distances, fréquences, etc. Commençons avec les 100 mots les plus fréquents, et une distance de Manhattan

```{r}
stylo(gui=TRUE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min =100, mfw.max = 100, mfw.incr=100,
      analysis.type = "CA", distance.measure = "manhattan",
      pca.visual.flavour = "classic")
```
## 2.2 Contrôler la fiabilité des résultats: répéter l'opération avec d'autres paramètres

Nous avons un premier cluster. Comment savoir s'il est fiable ? Une première solution est de répéter le même clacul, en augmentant le nombre des mots les plus fréquents et en changeant la distance.

Selon Evert, Proisl, Jannidi, Reger, Pielström, Schöch, Vitt (2017), 5000 mots MFW avec Delta serait la méthode la plus efficace. Nous travaillons avec des textes courts (nouvelles, contes), essayons donc Delta avec 2000 et 3000 mots.

```{r}
stylo(gui=FALSE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min = 2000, mfw.max = 100, mfw.incr=100,
      analysis.type = "CA", distance.measure = "delta",
      pca.visual.flavour = "classic")
```

Nous pouvons également demander à `Stylo` de répéter l'analyse de cluster un certain nombre de fois entre 100 et 1000.

Demandons à `Stylo` de refaire le calculs dix fois, en incrémentant de 100 mots à chaque fois.

```{r}
stylo(gui=FALSE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min = 100, mfw.max = 1000, mfw.incr=100,
      analysis.type = "CA", distance.measure = "delta",
      pca.visual.flavour = "classic")
```

## 2.3 Contrôler autrement les résultats: le *consensus tree*

Il est possible de représenter autrement la somme de ces informations: il s'agit du *concensus tree* ("arbre de consensus", en français).

C'est l'occasion de changer encore une fois la méthode de calcul de distance, pour vérifier les performances de chacune. Essayons d'abord avec une distance de Manhattan :

```{r}
stylo(gui=FALSE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min = 100, mfw.max = 1000, mfw.incr=100,
      analysis.type = "BCT", consensus.strength = 0.5,  distance.measure = "manhattan",
      pca.visual.flavour = "classic")
```


Et maintenant avec Delta :

```{r}
stylo(gui=FALSE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min = 100, mfw.max = 1000, mfw.incr=100,
      analysis.type = "BCT", consensus.strength = 0.5,  distance.measure = "delta",
      pca.visual.flavour = "classic")
```


# Résultats

Nous pouvons également créer un fichier avec les résultats et les consulter. Pour les stocker on crée l'objet "resultats":

```{r}
resultats <- stylo(gui=FALSE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min = 2000, mfw.max = 100, mfw.incr=100,
      analysis.type = "CA", distance.measure = "delta",
      pca.visual.flavour = "classic")
```
Pour les afficher :

```{r}
resultats
```
```{r}
summary(resultats)
```
Voici la liste des mots les plus fréquents :

```{r}
resultats$features
```

Ces mots sont classés du plus au moins fréquent dans le corpus. Nous pouvons voir leur fréquence par texte (pourcentage) avec la variable `table.with.all.freqs`


```{r}
resultats$table.with.all.freqs
```
Ces résultats permettent d'évaluer la distance entre chacun des textes de notre corpus. Cette fois les résultats sont accessibles avec le nom de notre variable suivi de `$distance.table` : 

```{r}
resultats$distance.table 
```
# 3 Autres visualisations

## 3.1 *Principal component analysis*

Un autre mode de visualisation est le *principal component analysis* ("Analyse en composantes principales" en français), qui permet lui aussi de spatialiser les résultats, selon une autre méthode de calcul :


```{r}
stylo(gui=FALSE, corpus.dir = "data/corpus", corpus.lang = "Spanish",
      analyzed.features = "w", mfw.min = 2000, mfw.max = 100, mfw.incr=100,
      analysis.type = "PCV",  distance.measure = "wurzburg",
      pca.visual.flavour = "classic")
```


